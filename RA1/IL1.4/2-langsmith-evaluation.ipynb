{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Evaluaci√≥n de Sistemas RAG con LangSmith\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "- Comprender la importancia de la evaluaci√≥n en sistemas RAG.\n",
    "- Configurar LangSmith para trazabilidad y evaluaci√≥n.\n",
    "- Crear un dataset de evaluaci√≥n con preguntas y respuestas de referencia.\n",
    "- Ejecutar evaluadores autom√°ticos para m√©tricas como relevancia y fidelidad.\n",
    "- Analizar los resultados de la evaluaci√≥n para optimizar el sistema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¬øQu√© es LangSmith?\n",
    "\n",
    "LangSmith es una plataforma de LangChain para la observabilidad, el monitoreo y la evaluaci√≥n de aplicaciones construidas con Modelos de Lenguaje Grandes (LLMs). Permite visualizar cada paso de una cadena o agente, analizar su rendimiento y evaluar la calidad de las respuestas de forma sistem√°tica.\n",
    "\n",
    "Para un sistema RAG, LangSmith nos ayuda a responder preguntas clave:\n",
    "- **Recuperaci√≥n (Retrieval)**: ¬øLos documentos que encontramos son relevantes para la pregunta?\n",
    "- **Generaci√≥n (Generation)**: ¬øLa respuesta generada es fiel a los documentos recuperados? ¬øResponde correctamente a la pregunta del usuario?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instalaci√≥n y Configuraci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade openai langsmith langchain langchain-openai langchain-community httpx scikit-learn numpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from langsmith import Client\n",
    "import json\n",
    "from typing import Dict, Any, List\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "print(\"‚úÖ Librer√≠as importadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuraci√≥n de Variables de Entorno\n",
    "\n",
    "Para que LangSmith capture las trazas de nuestra aplicaci√≥n, necesitamos configurar cuatro variables de entorno:\n",
    "1. `LANGCHAIN_TRACING_V2`: Se establece en `\"true\"` para activar la trazabilidad.\n",
    "2. `LANGCHAIN_API_KEY`: Tu clave de API de LangSmith. La puedes obtener en [smith.langchain.com](https://smith.langchain.com).\n",
    "3. `LANGCHAIN_PROJECT`: El nombre del proyecto bajo el cual se agrupar√°n las trazas. Esto es muy √∫til para organizar el trabajo.\n",
    "4. `OPENAI_API_KEY`: Tu clave de API de OpenAI para que el modelo de lenguaje funcione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar variables de entorno para LangSmith\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"RAG-Evaluation-Tutorial\"\n",
    "\n",
    "# Si usas LangSmith, aseg√∫rate de tener tu API key\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = \"tu_langsmith_api_key\"\n",
    "\n",
    "# Configurar API keys para el modelo de lenguaje\n",
    "# Si usas OpenAI directamente:\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"tu_openai_api_key\"\n",
    "\n",
    "# Si usas GitHub Models o Azure AI:\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"GITHUB_TOKEN\", \"\")\n",
    "os.environ[\"OPENAI_API_BASE\"] = os.getenv(\"GITHUB_BASE_URL\", \"https://models.inference.ai.azure.com\")\n",
    "\n",
    "# Inicializar el cliente de LangSmith\n",
    "try:\n",
    "    client = Client()\n",
    "    print(\"‚úÖ Cliente de LangSmith configurado\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è LangSmith no configurado: {e}\")\n",
    "    print(\"Continuando sin LangSmith...\")\n",
    "    client = None\n",
    "\n",
    "print(\"‚úÖ Variables de entorno configuradas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sistema RAG B√°sico\n",
    "\n",
    "Reutilizaremos el sistema RAG simple del notebook anterior. Este sistema utiliza una lista de documentos en memoria, una funci√≥n de recuperaci√≥n por palabras clave y un LLM para generar respuestas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base de documentos\n",
    "documents = [\n",
    "    \"La inteligencia artificial es una rama de la inform√°tica que busca crear m√°quinas capaces de realizar tareas que requieren inteligencia humana.\",\n",
    "    \"Los modelos de lenguaje grande (LLM) son sistemas de IA entrenados en enormes cantidades de texto para generar y comprender lenguaje natural.\",\n",
    "    \"RAG (Retrieval-Augmented Generation) combina la b√∫squeda de informaci√≥n relevante con la generaci√≥n de texto para producir respuestas m√°s precisas.\",\n",
    "    \"LangChain es un framework que facilita el desarrollo de aplicaciones con modelos de lenguaje, proporcionando herramientas para cadenas y agentes.\",\n",
    "    \"El prompt engineering es la pr√°ctica de dise√±ar instrucciones efectivas para obtener los mejores resultados de los modelos de IA.\"\n",
    "]\n",
    "\n",
    "# Cliente de OpenAI\n",
    "def initialize_client():\n",
    "    client = OpenAI(\n",
    "        base_url=os.getenv(\"OPENAI_API_BASE\", \"https://api.openai.com/v1\"),\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "    )\n",
    "    return client\n",
    "\n",
    "openai_client = initialize_client()\n",
    "\n",
    "print(f\"üìö Base de datos con {len(documents)} documentos cargada.\")\n",
    "print(\"‚úÖ Cliente OpenAI inicializado correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.run_helpers import traceable\n",
    "\n",
    "# Envolvemos las funciones con el decorador @traceable para que LangSmith las capture\n",
    "\n",
    "@traceable(name=\"Recuperacion de Documentos\")\n",
    "def simple_retrieval(query: str, documents: List[str]) -> List[str]:\n",
    "    \"\"\"Recupera documentos relevantes bas√°ndose en palabras clave.\"\"\"\n",
    "    relevant_docs = []\n",
    "    query_lower = query.lower()\n",
    "    for doc in documents:\n",
    "        if any(word in doc.lower() for word in query_lower.split()):\n",
    "            relevant_docs.append(doc)\n",
    "    return relevant_docs[:3]\n",
    "\n",
    "@traceable(name=\"Generacion de Respuesta\")\n",
    "def generate_response(client, query: str, context: str) -> str:\n",
    "    \"\"\"Genera una respuesta basada en el contexto proporcionado.\"\"\"\n",
    "    prompt = f\"\"\"Contexto:\n",
    "{context}\n",
    "\n",
    "Pregunta: {query}\n",
    "\n",
    "Responde bas√°ndote √∫nicamente en el contexto proporcionado. Si la informaci√≥n no est√° disponible en el contexto, indica que no puedes responder bas√°ndote en la informaci√≥n proporcionada.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error al generar respuesta: {e}\")\n",
    "        return \"Error al generar la respuesta.\"\n",
    "\n",
    "@traceable(name=\"Pipeline RAG Completo\")\n",
    "def rag_pipeline(query: str) -> Dict[str, Any]:\n",
    "    \"\"\"Pipeline completo de RAG que incluye recuperaci√≥n y generaci√≥n.\"\"\"\n",
    "    context_docs = simple_retrieval(query, documents)\n",
    "    context = \"\\n\\n\".join(context_docs)\n",
    "    answer = generate_response(openai_client, query, context)\n",
    "    return {\n",
    "        \"answer\": answer, \n",
    "        \"context\": context_docs,\n",
    "        \"query\": query\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Funciones del pipeline RAG definidas y trazables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba de Trazabilidad\n",
    "\n",
    "Ahora, si ejecutamos nuestro pipeline, LangSmith registrar√° la ejecuci√≥n completa, incluyendo los pasos intermedios que decoramos. Puedes ir a tu proyecto en LangSmith para ver la traza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba del sistema\n",
    "resultado = rag_pipeline(\"¬øQu√© es RAG?\")\n",
    "print(\"Respuesta:\")\n",
    "print(resultado[\"answer\"])\n",
    "print(\"\\nDocumentos recuperados:\")\n",
    "for i, doc in enumerate(resultado[\"context\"], 1):\n",
    "    print(f\"{i}. {doc[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creaci√≥n de un Dataset de Evaluaci√≥n\n",
    "\n",
    "Para evaluar nuestro sistema, necesitamos un \"ground truth\" (verdad fundamental), es decir, un conjunto de preguntas y las respuestas que consideramos correctas. En LangSmith, esto se gestiona a trav√©s de Datasets.\n",
    "\n",
    "### Tipos de Datasets de Evaluaci√≥n\n",
    "\n",
    "1. **Dataset de Exactitud**: Preguntas con respuestas espec√≠ficas y correctas\n",
    "2. **Dataset de Relevancia**: Eval√∫a si la respuesta es √∫til, aunque no sea exacta\n",
    "3. **Dataset de Fidelidad**: Verifica si la respuesta se basa en el contexto proporcionado\n",
    "4. **Dataset de Completitud**: Eval√∫a si la respuesta aborda todos los aspectos de la pregunta\n",
    "\n",
    "### Mejores Pr√°cticas para Crear Datasets:\n",
    "- **Diversidad**: Incluir diferentes tipos de preguntas\n",
    "- **Dificultad variada**: Preguntas f√°ciles, medianas y dif√≠ciles\n",
    "- **Casos edge**: Preguntas que podr√≠an confundir al sistema\n",
    "- **Actualizaci√≥n continua**: A√±adir nuevos ejemplos basados en casos reales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset de evaluaci√≥n m√°s completo y estructurado\n",
    "def create_comprehensive_evaluation_dataset():\n",
    "    \"\"\"Crea un dataset de evaluaci√≥n m√°s completo con diferentes categor√≠as.\"\"\"\n",
    "    \n",
    "    evaluation_examples = [\n",
    "        # Categor√≠a: Definiciones b√°sicas\n",
    "        {\n",
    "            \"inputs\": {\"query\": \"¬øQu√© es la inteligencia artificial?\"},\n",
    "            \"outputs\": {\"answer\": \"La inteligencia artificial es una rama de la inform√°tica que busca crear m√°quinas capaces de realizar tareas que requieren inteligencia humana.\"},\n",
    "            \"metadata\": {\"category\": \"definicion\", \"difficulty\": \"facil\", \"expected_docs\": 1}\n",
    "        },\n",
    "        {\n",
    "            \"inputs\": {\"query\": \"¬øQu√© son los LLM?\"},\n",
    "            \"outputs\": {\"answer\": \"Los modelos de lenguaje grande (LLM) son sistemas de IA entrenados en enormes cantidades de texto para generar y comprender lenguaje natural.\"},\n",
    "            \"metadata\": {\"category\": \"definicion\", \"difficulty\": \"facil\", \"expected_docs\": 1}\n",
    "        },\n",
    "        \n",
    "        # Categor√≠a: Aplicaciones y uso\n",
    "        {\n",
    "            \"inputs\": {\"query\": \"¬øPara qu√© sirve LangChain?\"},\n",
    "            \"outputs\": {\"answer\": \"LangChain es un framework que facilita el desarrollo de aplicaciones con modelos de lenguaje, proporcionando herramientas para cadenas y agentes.\"},\n",
    "            \"metadata\": {\"category\": \"aplicacion\", \"difficulty\": \"medio\", \"expected_docs\": 1}\n",
    "        },\n",
    "        {\n",
    "            \"inputs\": {\"query\": \"Explica qu√© es RAG y para qu√© se usa\"},\n",
    "            \"outputs\": {\"answer\": \"RAG (Retrieval-Augmented Generation) combina la b√∫squeda de informaci√≥n relevante con la generaci√≥n de texto para producir respuestas m√°s precisas.\"},\n",
    "            \"metadata\": {\"category\": \"aplicacion\", \"difficulty\": \"medio\", \"expected_docs\": 1}\n",
    "        },\n",
    "        \n",
    "        # Categor√≠a: T√©cnicas y m√©todos\n",
    "        {\n",
    "            \"inputs\": {\"query\": \"¬øQu√© es prompt engineering?\"},\n",
    "            \"outputs\": {\"answer\": \"El prompt engineering es la pr√°ctica de dise√±ar instrucciones efectivas para obtener los mejores resultados de los modelos de IA.\"},\n",
    "            \"metadata\": {\"category\": \"tecnica\", \"difficulty\": \"medio\", \"expected_docs\": 1}\n",
    "        },\n",
    "        \n",
    "        # Categor√≠a: Preguntas combinadas (m√°s dif√≠ciles)\n",
    "        {\n",
    "            \"inputs\": {\"query\": \"¬øC√≥mo se relacionan RAG y LangChain?\"},\n",
    "            \"outputs\": {\"answer\": \"LangChain facilita la implementaci√≥n de sistemas RAG proporcionando herramientas para combinar la b√∫squeda de informaci√≥n con la generaci√≥n de respuestas usando modelos de lenguaje.\"},\n",
    "            \"metadata\": {\"category\": \"relacion\", \"difficulty\": \"dificil\", \"expected_docs\": 2}\n",
    "        },\n",
    "        \n",
    "        # Categor√≠a: Casos edge - preguntas que no tienen respuesta en los docs\n",
    "        {\n",
    "            \"inputs\": {\"query\": \"¬øCu√°l es el precio de OpenAI GPT-4?\"},\n",
    "            \"outputs\": {\"answer\": \"No se puede responder esta pregunta bas√°ndose en la informaci√≥n proporcionada en el contexto.\"},\n",
    "            \"metadata\": {\"category\": \"sin_respuesta\", \"difficulty\": \"dificil\", \"expected_docs\": 0}\n",
    "        },\n",
    "        \n",
    "        # Categor√≠a: Preguntas ambiguas\n",
    "        {\n",
    "            \"inputs\": {\"query\": \"¬øQu√© es IA?\"},\n",
    "            \"outputs\": {\"answer\": \"La inteligencia artificial es una rama de la inform√°tica que busca crear m√°quinas capaces de realizar tareas que requieren inteligencia humana.\"},\n",
    "            \"metadata\": {\"category\": \"ambigua\", \"difficulty\": \"facil\", \"expected_docs\": 1}\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return evaluation_examples\n",
    "\n",
    "# Crear el dataset completo\n",
    "comprehensive_examples = create_comprehensive_evaluation_dataset()\n",
    "\n",
    "print(f\"üìä Dataset completo con {len(comprehensive_examples)} ejemplos creado\")\n",
    "print(\"\\nüìà Distribuci√≥n por categor√≠a:\")\n",
    "categories = {}\n",
    "difficulties = {}\n",
    "\n",
    "for example in comprehensive_examples:\n",
    "    cat = example[\"metadata\"][\"category\"]\n",
    "    diff = example[\"metadata\"][\"difficulty\"]\n",
    "    categories[cat] = categories.get(cat, 0) + 1\n",
    "    difficulties[diff] = difficulties.get(diff, 0) + 1\n",
    "\n",
    "for cat, count in categories.items():\n",
    "    print(f\"  ‚Ä¢ {cat}: {count} ejemplos\")\n",
    "    \n",
    "print(f\"\\nüéØ Distribuci√≥n por dificultad:\")\n",
    "for diff, count in difficulties.items():\n",
    "    print(f\"  ‚Ä¢ {diff}: {count} ejemplos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_dataset(examples):\n",
    "    \"\"\"Valida la calidad del dataset de evaluaci√≥n.\"\"\"\n",
    "    \n",
    "    print(\"üîç Validando dataset de evaluaci√≥n...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    # Validar estructura\n",
    "    for i, example in enumerate(examples):\n",
    "        # Verificar estructura requerida\n",
    "        if \"inputs\" not in example or \"outputs\" not in example:\n",
    "            issues.append(f\"Ejemplo {i+1}: Estructura inv√°lida (falta inputs/outputs)\")\n",
    "        \n",
    "        # Verificar que query no est√© vac√≠a\n",
    "        if \"query\" not in example.get(\"inputs\", {}) or not example[\"inputs\"][\"query\"].strip():\n",
    "            issues.append(f\"Ejemplo {i+1}: Query vac√≠a o faltante\")\n",
    "        \n",
    "        # Verificar que answer no est√© vac√≠a\n",
    "        if \"answer\" not in example.get(\"outputs\", {}) or not example[\"outputs\"][\"answer\"].strip():\n",
    "            issues.append(f\"Ejemplo {i+1}: Answer vac√≠a o faltante\")\n",
    "        \n",
    "        # Verificar longitud razonable\n",
    "        query = example.get(\"inputs\", {}).get(\"query\", \"\")\n",
    "        answer = example.get(\"outputs\", {}).get(\"answer\", \"\")\n",
    "        \n",
    "        if len(query) < 10:\n",
    "            issues.append(f\"Ejemplo {i+1}: Query muy corta (menos de 10 caracteres)\")\n",
    "        \n",
    "        if len(answer) < 20:\n",
    "            issues.append(f\"Ejemplo {i+1}: Answer muy corta (menos de 20 caracteres)\")\n",
    "        \n",
    "        if len(query) > 200:\n",
    "            issues.append(f\"Ejemplo {i+1}: Query muy larga (m√°s de 200 caracteres)\")\n",
    "    \n",
    "    # Verificar diversidad\n",
    "    queries = [ex[\"inputs\"][\"query\"].lower() for ex in examples]\n",
    "    unique_queries = set(queries)\n",
    "    \n",
    "    if len(unique_queries) < len(queries):\n",
    "        issues.append(\"Dataset contiene queries duplicadas\")\n",
    "    \n",
    "    # Verificar categor√≠as si existen metadatos\n",
    "    if examples and \"metadata\" in examples[0]:\n",
    "        categories = [ex[\"metadata\"].get(\"category\", \"unknown\") for ex in examples]\n",
    "        unique_categories = set(categories)\n",
    "        \n",
    "        if len(unique_categories) == 1:\n",
    "            issues.append(\"Dataset tiene poca diversidad de categor√≠as\")\n",
    "    \n",
    "    # Mostrar resultados\n",
    "    if issues:\n",
    "        print(\"‚ùå Problemas encontrados:\")\n",
    "        for issue in issues:\n",
    "            print(f\"  ‚Ä¢ {issue}\")\n",
    "    else:\n",
    "        print(\"‚úÖ Dataset v√°lido - No se encontraron problemas\")\n",
    "    \n",
    "    print(f\"\\nüìä Estad√≠sticas del dataset:\")\n",
    "    print(f\"  ‚Ä¢ Total de ejemplos: {len(examples)}\")\n",
    "    print(f\"  ‚Ä¢ Queries √∫nicas: {len(unique_queries)}\")\n",
    "    \n",
    "    if examples and \"metadata\" in examples[0]:\n",
    "        categories = {}\n",
    "        difficulties = {}\n",
    "        for ex in examples:\n",
    "            cat = ex[\"metadata\"].get(\"category\", \"unknown\")\n",
    "            diff = ex[\"metadata\"].get(\"difficulty\", \"unknown\")\n",
    "            categories[cat] = categories.get(cat, 0) + 1\n",
    "            difficulties[diff] = difficulties.get(diff, 0) + 1\n",
    "        \n",
    "        print(f\"  ‚Ä¢ Categor√≠as: {list(categories.keys())}\")\n",
    "        print(f\"  ‚Ä¢ Dificultades: {list(difficulties.keys())}\")\n",
    "    \n",
    "    return len(issues) == 0\n",
    "\n",
    "# Validar nuestro dataset\n",
    "is_valid = validate_dataset(comprehensive_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_langsmith_dataset_advanced():\n",
    "    \"\"\"Crear dataset en LangSmith con metadatos y gesti√≥n avanzada.\"\"\"\n",
    "    \n",
    "    if not client:\n",
    "        print(\"‚ö†Ô∏è LangSmith no configurado\")\n",
    "        return None\n",
    "    \n",
    "    dataset_name = \"RAG Evaluation - Comprehensive Dataset\"\n",
    "    description = \"\"\"Dataset completo para evaluaci√≥n de sistemas RAG incluyendo:\n",
    "    - Definiciones b√°sicas\n",
    "    - Aplicaciones y casos de uso\n",
    "    - T√©cnicas y m√©todos\n",
    "    - Preguntas combinadas\n",
    "    - Casos edge y preguntas sin respuesta\n",
    "    - Diferentes niveles de dificultad\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Eliminar dataset existente si existe\n",
    "        try:\n",
    "            existing_dataset = client.read_dataset(dataset_name=dataset_name)\n",
    "            client.delete_dataset(dataset_id=str(existing_dataset.id))\n",
    "            print(f\"üóëÔ∏è Dataset existente '{dataset_name}' eliminado.\")\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Crear nuevo dataset\n",
    "        dataset = client.create_dataset(\n",
    "            dataset_name=dataset_name,\n",
    "            description=description,\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Dataset '{dataset_name}' creado con ID: {dataset.id}\")\n",
    "        \n",
    "        # A√±adir ejemplos con metadatos\n",
    "        for i, example in enumerate(comprehensive_examples):\n",
    "            try:\n",
    "                client.create_example(\n",
    "                    inputs=example[\"inputs\"],\n",
    "                    outputs=example[\"outputs\"],\n",
    "                    metadata=example[\"metadata\"],  # Incluir metadatos\n",
    "                    dataset_id=dataset.id,\n",
    "                )\n",
    "                print(f\"  üìù Ejemplo {i+1} a√±adido: {example['metadata']['category']}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ùå Error a√±adiendo ejemplo {i+1}: {e}\")\n",
    "        \n",
    "        print(f\"‚úÖ {len(comprehensive_examples)} ejemplos a√±adidos exitosamente\")\n",
    "        \n",
    "        return dataset\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creando dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "# Crear dataset avanzado\n",
    "advanced_dataset = create_langsmith_dataset_advanced()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluaci√≥n Manual (Sin LangSmith)\n",
    "\n",
    "Si no tienes LangSmith configurado, puedes realizar una evaluaci√≥n b√°sica manual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_evaluation():\n",
    "    \"\"\"Evaluaci√≥n simple sin LangSmith\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    print(\"üîç Ejecutando evaluaci√≥n manual...\\n\")\n",
    "    \n",
    "    for i, example in enumerate(comprehensive_examples, 1):\n",
    "        query = example[\"inputs\"][\"query\"]\n",
    "        expected = example[\"outputs\"][\"answer\"]\n",
    "        category = example[\"metadata\"][\"category\"]\n",
    "        difficulty = example[\"metadata\"][\"difficulty\"]\n",
    "        \n",
    "        print(f\"Ejemplo {i}/{len(comprehensive_examples)} - {category} ({difficulty})\")\n",
    "        print(f\"Pregunta: {query}\")\n",
    "        \n",
    "        # Ejecutar el pipeline\n",
    "        result = rag_pipeline(query)\n",
    "        generated = result[\"answer\"]\n",
    "        \n",
    "        print(f\"Respuesta esperada: {expected}\")\n",
    "        print(f\"Respuesta generada: {generated}\")\n",
    "        \n",
    "        # Evaluaci√≥n b√°sica de similitud (palabras en com√∫n)\n",
    "        expected_words = set(expected.lower().split())\n",
    "        generated_words = set(generated.lower().split())\n",
    "        \n",
    "        if len(expected_words) > 0:\n",
    "            similarity = len(expected_words.intersection(generated_words)) / len(expected_words.union(generated_words))\n",
    "        else:\n",
    "            similarity = 0\n",
    "        \n",
    "        # Evaluaci√≥n de recuperaci√≥n\n",
    "        context_retrieved = len(result[\"context\"]) > 0\n",
    "        expected_docs = example[\"metadata\"][\"expected_docs\"]\n",
    "        retrieval_success = len(result[\"context\"]) >= expected_docs if expected_docs > 0 else len(result[\"context\"]) == 0\n",
    "        \n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"expected\": expected,\n",
    "            \"generated\": generated,\n",
    "            \"similarity\": similarity,\n",
    "            \"context_docs\": len(result[\"context\"]),\n",
    "            \"category\": category,\n",
    "            \"difficulty\": difficulty,\n",
    "            \"retrieval_success\": retrieval_success\n",
    "        })\n",
    "        \n",
    "        print(f\"Similitud: {similarity:.2f}\")\n",
    "        print(f\"Documentos recuperados: {len(result['context'])}\")\n",
    "        print(f\"Recuperaci√≥n exitosa: {'‚úÖ' if retrieval_success else '‚ùå'}\")\n",
    "        print(\"-\" * 80 + \"\\n\")\n",
    "    \n",
    "    # Resumen por categor√≠a\n",
    "    print(\"üìä Resumen por Categor√≠a:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    category_stats = {}\n",
    "    for r in results:\n",
    "        cat = r[\"category\"]\n",
    "        if cat not in category_stats:\n",
    "            category_stats[cat] = {\"total\": 0, \"similarity_sum\": 0, \"retrieval_success\": 0}\n",
    "        \n",
    "        category_stats[cat][\"total\"] += 1\n",
    "        category_stats[cat][\"similarity_sum\"] += r[\"similarity\"]\n",
    "        if r[\"retrieval_success\"]:\n",
    "            category_stats[cat][\"retrieval_success\"] += 1\n",
    "    \n",
    "    for cat, stats in category_stats.items():\n",
    "        avg_similarity = stats[\"similarity_sum\"] / stats[\"total\"]\n",
    "        retrieval_rate = stats[\"retrieval_success\"] / stats[\"total\"]\n",
    "        print(f\"{cat}: Similitud={avg_similarity:.2f}, Recuperaci√≥n={retrieval_rate:.2f}\")\n",
    "    \n",
    "    # Resumen general\n",
    "    avg_similarity = sum(r[\"similarity\"] for r in results) / len(results)\n",
    "    retrieval_rate = sum(r[\"retrieval_success\"] for r in results) / len(results)\n",
    "    \n",
    "    print(f\"\\nüìà Resumen General:\")\n",
    "    print(f\"Similitud promedio: {avg_similarity:.2f}\")\n",
    "    print(f\"Tasa de recuperaci√≥n exitosa: {retrieval_rate:.2f}\")\n",
    "    print(f\"Total de ejemplos: {len(results)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Ejecutar evaluaci√≥n manual\n",
    "manual_results = simple_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluaci√≥n con LangSmith (Si est√° configurado)\n",
    "\n",
    "Si tienes LangSmith configurado, puedes ejecutar una evaluaci√≥n m√°s sofisticada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solo ejecutar si LangSmith est√° configurado\n",
    "if client and advanced_dataset:\n",
    "    try:\n",
    "        from langsmith.evaluation import evaluate\n",
    "        from langchain_openai import ChatOpenAI\n",
    "        \n",
    "        print(\"üöÄ Configurando evaluaci√≥n con LangSmith...\")\n",
    "        \n",
    "        # Modelo de lenguaje para la evaluaci√≥n\n",
    "        llm = ChatOpenAI(\n",
    "            model=\"gpt-4o\",\n",
    "            temperature=0.1,\n",
    "            base_url=os.getenv(\"OPENAI_API_BASE\", \"https://api.openai.com/v1\"),\n",
    "            api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "        )\n",
    "        \n",
    "        # Funci√≥n target simplificada\n",
    "        def target_function(inputs: Dict[str, Any]) -> str:\n",
    "            \"\"\"Funci√≥n target que retorna solo la respuesta como string.\"\"\"\n",
    "            result = rag_pipeline(inputs[\"query\"])\n",
    "            return result[\"answer\"]  # Retornar solo la respuesta como string\n",
    "        \n",
    "        print(\"üîÑ Ejecutando evaluaci√≥n sin evaluadores autom√°ticos...\")\n",
    "        \n",
    "        # ENFOQUE SIMPLIFICADO: Solo ejecutar y capturar las trazas\n",
    "        experiment_results = evaluate(\n",
    "            target_function,\n",
    "            data=\"RAG Evaluation - Comprehensive Dataset\",\n",
    "            evaluators=[],  # Sin evaluadores autom√°ticos por ahora\n",
    "            experiment_prefix=\"RAG Basico - Trace Only\",\n",
    "            metadata={\n",
    "                \"version\": \"2.2.0\", \n",
    "                \"model\": \"gpt-4o\", \n",
    "                \"dataset\": \"comprehensive\", \n",
    "                \"approach\": \"trace_only\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Evaluaci√≥n de trazabilidad completada exitosamente.\")\n",
    "        print(\"üîó Revisa las trazas en la plataforma LangSmith.\")\n",
    "        print(\"üìù Las respuestas est√°n capturadas - puedes evaluar manualmente en la interfaz\")\n",
    "        \n",
    "        # Mostrar informaci√≥n del experimento\n",
    "        if hasattr(experiment_results, 'experiment_name'):\n",
    "            print(f\"üìä Nombre del experimento: {experiment_results.experiment_name}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en evaluaci√≥n con LangSmith: {e}\")\n",
    "        print(\"üîÑ Continuando con evaluaci√≥n manual local...\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è LangSmith no configurado, usando evaluaci√≥n manual √∫nicamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_manual_evaluation():\n",
    "    \"\"\"Evaluaci√≥n manual completa con m√∫ltiples m√©tricas.\"\"\"\n",
    "    print(\"üîç Ejecutando evaluaci√≥n manual completa...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results = []\n",
    "    total_time = 0\n",
    "    \n",
    "    for i, example in enumerate(comprehensive_examples, 1):\n",
    "        query = example[\"inputs\"][\"query\"]\n",
    "        expected = example[\"outputs\"][\"answer\"]\n",
    "        category = example[\"metadata\"][\"category\"]\n",
    "        difficulty = example[\"metadata\"][\"difficulty\"]\n",
    "        \n",
    "        print(f\"\\nüìù Ejemplo {i}/{len(comprehensive_examples)}\")\n",
    "        print(f\"Categor√≠a: {category} | Dificultad: {difficulty}\")\n",
    "        print(f\"Pregunta: {query}\")\n",
    "        \n",
    "        # Medir tiempo de respuesta\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Ejecutar pipeline\n",
    "        result = rag_pipeline(query)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        response_time = end_time - start_time\n",
    "        total_time += response_time\n",
    "        \n",
    "        generated = result[\"answer\"]\n",
    "        context_docs = result[\"context\"]\n",
    "        \n",
    "        print(f\"Respuesta generada: {generated}\")\n",
    "        print(f\"Respuesta esperada: {expected}\")\n",
    "        print(f\"Documentos recuperados: {len(context_docs)}\")\n",
    "        print(f\"Tiempo de respuesta: {response_time:.2f}s\")\n",
    "        \n",
    "        # M√∫ltiples m√©tricas de evaluaci√≥n\n",
    "        \n",
    "        # 1. Similitud de palabras (Jaccard)\n",
    "        expected_words = set(expected.lower().split())\n",
    "        generated_words = set(generated.lower().split())\n",
    "        \n",
    "        if len(expected_words) > 0:\n",
    "            jaccard_similarity = len(expected_words.intersection(generated_words)) / len(expected_words.union(generated_words))\n",
    "        else:\n",
    "            jaccard_similarity = 0\n",
    "        \n",
    "        # 2. Contenci√≥n (¬øLa respuesta contiene conceptos clave?)\n",
    "        key_concepts = [\"inteligencia artificial\", \"llm\", \"rag\", \"langchain\", \"prompt engineering\"]\n",
    "        concepts_in_expected = [concept for concept in key_concepts if concept in expected.lower()]\n",
    "        concepts_in_generated = [concept for concept in key_concepts if concept in generated.lower()]\n",
    "        \n",
    "        concept_coverage = len(concepts_in_generated) / max(1, len(concepts_in_expected)) if concepts_in_expected else 1\n",
    "        \n",
    "        # 3. Evaluaci√≥n de recuperaci√≥n\n",
    "        expected_docs = example[\"metadata\"][\"expected_docs\"]\n",
    "        retrieval_success = len(context_docs) >= expected_docs if expected_docs > 0 else len(context_docs) == 0\n",
    "        \n",
    "        # 4. Longitud de respuesta (relativa a la esperada)\n",
    "        length_ratio = len(generated) / max(1, len(expected))\n",
    "        length_score = 1.0 if 0.5 <= length_ratio <= 2.0 else max(0, 1 - abs(length_ratio - 1))\n",
    "        \n",
    "        # 5. Detecci√≥n de \"no puedo responder\"\n",
    "        no_answer_phrases = [\"no puedo responder\", \"no se puede responder\", \"no est√° disponible\", \"no tengo informaci√≥n\"]\n",
    "        contains_no_answer = any(phrase in generated.lower() for phrase in no_answer_phrases)\n",
    "        should_have_no_answer = expected_docs == 0\n",
    "        no_answer_correct = (contains_no_answer and should_have_no_answer) or (not contains_no_answer and not should_have_no_answer)\n",
    "        \n",
    "        # Puntuaci√≥n compuesta\n",
    "        composite_score = (\n",
    "            jaccard_similarity * 0.3 +\n",
    "            concept_coverage * 0.2 +\n",
    "            (1.0 if retrieval_success else 0.0) * 0.2 +\n",
    "            length_score * 0.15 +\n",
    "            (1.0 if no_answer_correct else 0.0) * 0.15\n",
    "        )\n",
    "        \n",
    "        result_data = {\n",
    "            \"query\": query,\n",
    "            \"expected\": expected,\n",
    "            \"generated\": generated,\n",
    "            \"category\": category,\n",
    "            \"difficulty\": difficulty,\n",
    "            \"context_docs\": len(context_docs),\n",
    "            \"response_time\": response_time,\n",
    "            \"jaccard_similarity\": jaccard_similarity,\n",
    "            \"concept_coverage\": concept_coverage,\n",
    "            \"retrieval_success\": retrieval_success,\n",
    "            \"length_score\": length_score,\n",
    "            \"no_answer_correct\": no_answer_correct,\n",
    "            \"composite_score\": composite_score\n",
    "        }\n",
    "        \n",
    "        results.append(result_data)\n",
    "        \n",
    "        print(f\"Similitud Jaccard: {jaccard_similarity:.3f}\")\n",
    "        print(f\"Cobertura conceptos: {concept_coverage:.3f}\")\n",
    "        print(f\"Recuperaci√≥n exitosa: {'‚úÖ' if retrieval_success else '‚ùå'}\")\n",
    "        print(f\"Puntuaci√≥n compuesta: {composite_score:.3f}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    # An√°lisis de resultados\n",
    "    print(f\"\\nüìä RESUMEN DE EVALUACI√ìN COMPLETA\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    avg_jaccard = sum(r[\"jaccard_similarity\"] for r in results) / len(results)\n",
    "    avg_concept = sum(r[\"concept_coverage\"] for r in results) / len(results)\n",
    "    avg_composite = sum(r[\"composite_score\"] for r in results) / len(results)\n",
    "    avg_time = total_time / len(results)\n",
    "    retrieval_rate = sum(r[\"retrieval_success\"] for r in results) / len(results)\n",
    "    \n",
    "    print(f\"Similitud Jaccard promedio: {avg_jaccard:.3f}\")\n",
    "    print(f\"Cobertura de conceptos promedio: {avg_concept:.3f}\")\n",
    "    print(f\"Tasa de recuperaci√≥n exitosa: {retrieval_rate:.3f}\")\n",
    "    print(f\"Tiempo promedio de respuesta: {avg_time:.2f}s\")\n",
    "    print(f\"Puntuaci√≥n compuesta promedio: {avg_composite:.3f}\")\n",
    "    \n",
    "    # An√°lisis por categor√≠a\n",
    "    print(f\"\\nüìà An√°lisis por categor√≠a:\")\n",
    "    category_stats = {}\n",
    "    for r in results:\n",
    "        cat = r[\"category\"]\n",
    "        if cat not in category_stats:\n",
    "            category_stats[cat] = []\n",
    "        category_stats[cat].append(r[\"composite_score\"])\n",
    "    \n",
    "    for cat, scores in category_stats.items():\n",
    "        avg_score = sum(scores) / len(scores)\n",
    "        print(f\"  {cat}: {avg_score:.3f} ({len(scores)} ejemplos)\")\n",
    "    \n",
    "    # An√°lisis por dificultad\n",
    "    print(f\"\\nüéØ An√°lisis por dificultad:\")\n",
    "    difficulty_stats = {}\n",
    "    for r in results:\n",
    "        diff = r[\"difficulty\"]\n",
    "        if diff not in difficulty_stats:\n",
    "            difficulty_stats[diff] = []\n",
    "        difficulty_stats[diff].append(r[\"composite_score\"])\n",
    "    \n",
    "    for diff, scores in difficulty_stats.items():\n",
    "        avg_score = sum(scores) / len(scores)\n",
    "        print(f\"  {diff}: {avg_score:.3f} ({len(scores)} ejemplos)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Ejecutar evaluaci√≥n manual completa\n",
    "comprehensive_results = comprehensive_manual_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. An√°lisis de Resultados y Mejoras\n",
    "\n",
    "Bas√°ndose en los resultados de la evaluaci√≥n, aqu√≠ tienes algunas √°reas comunes de mejora para sistemas RAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results_detailed(results):\n",
    "    \"\"\"Analiza los resultados y sugiere mejoras espec√≠ficas.\"\"\"\n",
    "    print(\"üìà An√°lisis Detallado de Resultados:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if not results:\n",
    "        print(\"‚ö†Ô∏è No hay resultados para analizar\")\n",
    "        return\n",
    "    \n",
    "    # Estad√≠sticas generales\n",
    "    avg_composite = sum(r[\"composite_score\"] for r in results) / len(results)\n",
    "    avg_jaccard = sum(r[\"jaccard_similarity\"] for r in results) / len(results)\n",
    "    retrieval_rate = sum(r[\"retrieval_success\"] for r in results) / len(results)\n",
    "    avg_time = sum(r[\"response_time\"] for r in results) / len(results)\n",
    "    \n",
    "    print(f\"Puntuaci√≥n compuesta promedio: {avg_composite:.3f}\")\n",
    "    print(f\"Similitud Jaccard promedio: {avg_jaccard:.3f}\")\n",
    "    print(f\"Tasa de recuperaci√≥n exitosa: {retrieval_rate:.3f}\")\n",
    "    print(f\"Tiempo promedio de respuesta: {avg_time:.2f}s\")\n",
    "    \n",
    "    # Identificar problemas espec√≠ficos\n",
    "    problems = []\n",
    "    \n",
    "    if avg_composite < 0.6:\n",
    "        problems.append(\"Puntuaci√≥n general baja\")\n",
    "    if avg_jaccard < 0.3:\n",
    "        problems.append(\"Baja similitud en las respuestas\")\n",
    "    if retrieval_rate < 0.7:\n",
    "        problems.append(\"Problemas en la recuperaci√≥n de documentos\")\n",
    "    if avg_time > 5.0:\n",
    "        problems.append(\"Tiempo de respuesta lento\")\n",
    "    \n",
    "    # Encontrar casos problem√°ticos\n",
    "    low_performance = [r for r in results if r[\"composite_score\"] < 0.4]\n",
    "    retrieval_failures = [r for r in results if not r[\"retrieval_success\"]]\n",
    "    \n",
    "    print(f\"\\nüö® Problemas identificados:\")\n",
    "    if problems:\n",
    "        for problem in problems:\n",
    "            print(f\"  ‚Ä¢ {problem}\")\n",
    "    else:\n",
    "        print(\"  ‚úÖ No se identificaron problemas mayores\")\n",
    "    \n",
    "    if low_performance:\n",
    "        print(f\"\\nüìâ Casos de bajo rendimiento ({len(low_performance)}):\")\n",
    "        for case in low_performance[:3]:  # Mostrar solo los primeros 3\n",
    "            print(f\"  ‚Ä¢ '{case['query'][:50]}...' (Score: {case['composite_score']:.3f})\")\n",
    "    \n",
    "    if retrieval_failures:\n",
    "        print(f\"\\nüîç Fallos de recuperaci√≥n ({len(retrieval_failures)}):\")\n",
    "        for case in retrieval_failures[:3]:\n",
    "            print(f\"  ‚Ä¢ '{case['query'][:50]}...' (Docs: {case['context_docs']})\")\n",
    "    \n",
    "    # Recomendaciones espec√≠ficas\n",
    "    print(f\"\\nüí° Recomendaciones espec√≠ficas:\")\n",
    "    \n",
    "    if avg_jaccard < 0.3:\n",
    "        print(\"  üîß Mejorar la generaci√≥n de respuestas:\")\n",
    "        print(\"    - Refinar el prompt de generaci√≥n\")\n",
    "        print(\"    - Usar ejemplos few-shot\")\n",
    "        print(\"    - Ajustar la temperatura del modelo\")\n",
    "    \n",
    "    if retrieval_rate < 0.7:\n",
    "        print(\"  üîß Mejorar la recuperaci√≥n:\")\n",
    "        print(\"    - Implementar b√∫squeda sem√°ntica con embeddings\")\n",
    "        print(\"    - Ajustar umbrales de similitud\")\n",
    "        print(\"    - Expandir la base de documentos\")\n",
    "    \n",
    "    if avg_time > 3.0:\n",
    "        print(\"  üîß Optimizar rendimiento:\")\n",
    "        print(\"    - Usar modelos m√°s r√°pidos para recuperaci√≥n\")\n",
    "        print(\"    - Implementar cache de respuestas\")\n",
    "        print(\"    - Paralelizar operaciones cuando sea posible\")\n",
    "    \n",
    "    print(f\"\\nüîß Estrategias generales de mejora:\")\n",
    "    print(\"1. Implementar embeddings sem√°nticos (OpenAI, Sentence Transformers)\")\n",
    "    print(\"2. Usar re-ranking de documentos recuperados\")\n",
    "    print(\"3. Implementar query expansion y refinement\")\n",
    "    print(\"4. A√±adir evaluaci√≥n de confianza en las respuestas\")\n",
    "    print(\"5. Crear un sistema de feedback para mejorar continuamente\")\n",
    "\n",
    "# Analizar resultados detallados\n",
    "analyze_results_detailed(comprehensive_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Ejemplo de Mejora: Sistema RAG con Embeddings\n",
    "\n",
    "Como ejemplo de mejora, aqu√≠ tienes una versi√≥n mejorada que usa embeddings sem√°nticos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Versi√≥n mejorada con TF-IDF (como alternativa a embeddings reales)\n",
    "class ImprovedRAGSystem:\n",
    "    def __init__(self, documents):\n",
    "        self.documents = documents\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            stop_words='english',\n",
    "            ngram_range=(1, 2),  # Usar unigramas y bigramas\n",
    "            max_features=1000,   # Limitar caracter√≠sticas\n",
    "            lowercase=True\n",
    "        )\n",
    "        self.doc_vectors = self.vectorizer.fit_transform(documents)\n",
    "        print(f\"‚úÖ Sistema mejorado inicializado con {len(documents)} documentos\")\n",
    "    \n",
    "    @traceable(name=\"Recuperacion Semantica Mejorada\")\n",
    "    def semantic_retrieval(self, query, top_k=3, threshold=0.1):\n",
    "        \"\"\"Recuperaci√≥n basada en similitud sem√°ntica usando TF-IDF.\"\"\"\n",
    "        query_vector = self.vectorizer.transform([query])\n",
    "        similarities = cosine_similarity(query_vector, self.doc_vectors)[0]\n",
    "        \n",
    "        # Obtener los √≠ndices de los documentos m√°s similares\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "        \n",
    "        relevant_docs = []\n",
    "        similarity_scores = []\n",
    "        \n",
    "        for i in top_indices:\n",
    "            if similarities[i] > threshold:\n",
    "                relevant_docs.append(self.documents[i])\n",
    "                similarity_scores.append(similarities[i])\n",
    "        \n",
    "        return relevant_docs, similarity_scores\n",
    "    \n",
    "    @traceable(name=\"Generacion de Respuesta Mejorada\")\n",
    "    def generate_enhanced_response(self, query, context_docs, similarity_scores):\n",
    "        \"\"\"Genera respuesta con contexto enriquecido.\"\"\"\n",
    "        if not context_docs:\n",
    "            return \"No se encontr√≥ informaci√≥n relevante para responder la pregunta.\"\n",
    "        \n",
    "        # Crear contexto enriquecido con puntuaciones de similitud\n",
    "        enriched_context = \"Informaci√≥n relevante encontrada:\\n\\n\"\n",
    "        for i, (doc, score) in enumerate(zip(context_docs, similarity_scores), 1):\n",
    "            enriched_context += f\"Documento {i} (relevancia: {score:.3f}):\\n{doc}\\n\\n\"\n",
    "        \n",
    "        prompt = f\"\"\"Bas√°ndote en la informaci√≥n proporcionada, responde la siguiente pregunta de manera precisa y completa.\n",
    "\n",
    "{enriched_context}\n",
    "\n",
    "Pregunta: {query}\n",
    "\n",
    "Instrucciones:\n",
    "- Usa √∫nicamente la informaci√≥n proporcionada en los documentos\n",
    "- Si la informaci√≥n no est√° disponible, indica claramente que no puedes responder\n",
    "- S√© preciso y directo en tu respuesta\n",
    "- Si hay m√∫ltiples documentos relevantes, sintetiza la informaci√≥n\n",
    "\n",
    "Respuesta:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = openai_client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.1,\n",
    "                max_tokens=300\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"Error al generar respuesta: {e}\")\n",
    "            return \"Error al generar la respuesta.\"\n",
    "    \n",
    "    @traceable(name=\"Pipeline RAG Mejorado Completo\")\n",
    "    def improved_rag_pipeline(self, query):\n",
    "        \"\"\"Pipeline RAG mejorado con recuperaci√≥n sem√°ntica y generaci√≥n enriquecida.\"\"\"\n",
    "        context_docs, similarity_scores = self.semantic_retrieval(query)\n",
    "        answer = self.generate_enhanced_response(query, context_docs, similarity_scores)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"context\": context_docs,\n",
    "            \"similarity_scores\": similarity_scores,\n",
    "            \"query\": query,\n",
    "            \"num_docs_retrieved\": len(context_docs)\n",
    "        }\n",
    "\n",
    "# Crear sistema mejorado\n",
    "improved_system = ImprovedRAGSystem(documents)\n",
    "\n",
    "print(\"‚úÖ Sistema RAG mejorado creado con TF-IDF sem√°ntico\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_systems_comprehensive():\n",
    "    \"\"\"Compara el sistema original con el mejorado de forma detallada.\"\"\"\n",
    "    print(\"üîç Comparaci√≥n Comprensiva de Sistemas RAG\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    test_queries = [\n",
    "        \"¬øQu√© es la inteligencia artificial?\",\n",
    "        \"¬øC√≥mo funciona RAG?\", \n",
    "        \"Explica los modelos de lenguaje\",\n",
    "        \"¬øQu√© herramientas usa LangChain?\",\n",
    "        \"¬øQu√© es el deep learning?\"  # Esta no deber√≠a tener respuesta clara\n",
    "    ]\n",
    "    \n",
    "    comparison_results = []\n",
    "    \n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        print(f\"\\nüîç Prueba {i}: '{query}'\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Medir tiempo sistema original\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        original_result = rag_pipeline(query)\n",
    "        original_time = time.time() - start_time\n",
    "        \n",
    "        # Medir tiempo sistema mejorado\n",
    "        start_time = time.time()\n",
    "        improved_result = improved_system.improved_rag_pipeline(query)\n",
    "        improved_time = time.time() - start_time\n",
    "        \n",
    "        print(\"Sistema Original:\")\n",
    "        print(f\"  Respuesta: {original_result['answer'][:200]}...\")\n",
    "        print(f\"  Documentos recuperados: {len(original_result['context'])}\")\n",
    "        print(f\"  Tiempo: {original_time:.3f}s\")\n",
    "        \n",
    "        print(\"\\nSistema Mejorado:\")\n",
    "        print(f\"  Respuesta: {improved_result['answer'][:200]}...\")\n",
    "        print(f\"  Documentos recuperados: {len(improved_result['context'])}\")\n",
    "        if improved_result.get('similarity_scores'):\n",
    "            avg_similarity = sum(improved_result['similarity_scores']) / len(improved_result['similarity_scores'])\n",
    "            print(f\"  Similitud promedio: {avg_similarity:.3f}\")\n",
    "        print(f\"  Tiempo: {improved_time:.3f}s\")\n",
    "        \n",
    "        # Calcular m√©tricas de comparaci√≥n\n",
    "        original_length = len(original_result['answer'])\n",
    "        improved_length = len(improved_result['answer'])\n",
    "        \n",
    "        comparison_data = {\n",
    "            \"query\": query,\n",
    "            \"original_docs\": len(original_result['context']),\n",
    "            \"improved_docs\": len(improved_result['context']),\n",
    "            \"original_time\": original_time,\n",
    "            \"improved_time\": improved_time,\n",
    "            \"original_length\": original_length,\n",
    "            \"improved_length\": improved_length,\n",
    "            \"time_improvement\": ((original_time - improved_time) / original_time * 100) if original_time > 0 else 0\n",
    "        }\n",
    "        \n",
    "        comparison_results.append(comparison_data)\n",
    "        \n",
    "        print(f\"Mejora en tiempo: {comparison_data['time_improvement']:+.1f}%\")\n",
    "    \n",
    "    # Resumen de comparaci√≥n\n",
    "    print(f\"\\nüìä RESUMEN DE COMPARACI√ìN\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    avg_original_time = sum(r[\"original_time\"] for r in comparison_results) / len(comparison_results)\n",
    "    avg_improved_time = sum(r[\"improved_time\"] for r in comparison_results) / len(comparison_results)\n",
    "    avg_time_improvement = sum(r[\"time_improvement\"] for r in comparison_results) / len(comparison_results)\n",
    "    \n",
    "    print(f\"Tiempo promedio original: {avg_original_time:.3f}s\")\n",
    "    print(f\"Tiempo promedio mejorado: {avg_improved_time:.3f}s\")\n",
    "    print(f\"Mejora promedio en tiempo: {avg_time_improvement:+.1f}%\")\n",
    "    \n",
    "    return comparison_results\n",
    "\n",
    "# Ejecutar comparaci√≥n\n",
    "comparison_data = compare_systems_comprehensive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_improved_system_complete():\n",
    "    \"\"\"Eval√∫a el sistema mejorado con el mismo dataset usando las mismas m√©tricas.\"\"\"\n",
    "    print(\"üöÄ Evaluando Sistema RAG Mejorado...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    improved_results = []\n",
    "    \n",
    "    for i, example in enumerate(comprehensive_examples, 1):\n",
    "        query = example[\"inputs\"][\"query\"]\n",
    "        expected = example[\"outputs\"][\"answer\"]\n",
    "        category = example[\"metadata\"][\"category\"]\n",
    "        difficulty = example[\"metadata\"][\"difficulty\"]\n",
    "        \n",
    "        print(f\"Evaluando ejemplo {i}/{len(comprehensive_examples)}: {category}\")\n",
    "        \n",
    "        # Ejecutar pipeline mejorado\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        result = improved_system.improved_rag_pipeline(query)\n",
    "        response_time = time.time() - start_time\n",
    "        \n",
    "        generated = result[\"answer\"]\n",
    "        context_docs = result[\"context\"]\n",
    "        similarity_scores = result.get(\"similarity_scores\", [])\n",
    "        \n",
    "        # Aplicar las mismas m√©tricas que la evaluaci√≥n manual\n",
    "        expected_words = set(expected.lower().split())\n",
    "        generated_words = set(generated.lower().split())\n",
    "        \n",
    "        if len(expected_words) > 0:\n",
    "            jaccard_similarity = len(expected_words.intersection(generated_words)) / len(expected_words.union(generated_words))\n",
    "        else:\n",
    "            jaccard_similarity = 0\n",
    "        \n",
    "        # Cobertura de conceptos\n",
    "        key_concepts = [\"inteligencia artificial\", \"llm\", \"rag\", \"langchain\", \"prompt engineering\"]\n",
    "        concepts_in_expected = [concept for concept in key_concepts if concept in expected.lower()]\n",
    "        concepts_in_generated = [concept for concept in key_concepts if concept in generated.lower()]\n",
    "        concept_coverage = len(concepts_in_generated) / max(1, len(concepts_in_expected)) if concepts_in_expected else 1\n",
    "        \n",
    "        # Evaluaci√≥n de recuperaci√≥n\n",
    "        expected_docs = example[\"metadata\"][\"expected_docs\"]\n",
    "        retrieval_success = len(context_docs) >= expected_docs if expected_docs > 0 else len(context_docs) == 0\n",
    "        \n",
    "        # Longitud de respuesta\n",
    "        length_ratio = len(generated) / max(1, len(expected))\n",
    "        length_score = 1.0 if 0.5 <= length_ratio <= 2.0 else max(0, 1 - abs(length_ratio - 1))\n",
    "        \n",
    "        # Detecci√≥n de \"no puedo responder\"\n",
    "        no_answer_phrases = [\"no puedo responder\", \"no se puede responder\", \"no est√° disponible\", \"no tengo informaci√≥n\"]\n",
    "        contains_no_answer = any(phrase in generated.lower() for phrase in no_answer_phrases)\n",
    "        should_have_no_answer = expected_docs == 0\n",
    "        no_answer_correct = (contains_no_answer and should_have_no_answer) or (not contains_no_answer and not should_have_no_answer)\n",
    "        \n",
    "        # Puntuaci√≥n compuesta (misma f√≥rmula)\n",
    "        composite_score = (\n",
    "            jaccard_similarity * 0.3 +\n",
    "            concept_coverage * 0.2 +\n",
    "            (1.0 if retrieval_success else 0.0) * 0.2 +\n",
    "            length_score * 0.15 +\n",
    "            (1.0 if no_answer_correct else 0.0) * 0.15\n",
    "        )\n",
    "        \n",
    "        # M√©trica adicional: calidad de recuperaci√≥n sem√°ntica\n",
    "        avg_semantic_similarity = sum(similarity_scores) / len(similarity_scores) if similarity_scores else 0\n",
    "        \n",
    "        result_data = {\n",
    "            \"query\": query,\n",
    "            \"expected\": expected,\n",
    "            \"generated\": generated,\n",
    "            \"category\": category,\n",
    "            \"difficulty\": difficulty,\n",
    "            \"context_docs\": len(context_docs),\n",
    "            \"response_time\": response_time,\n",
    "            \"jaccard_similarity\": jaccard_similarity,\n",
    "            \"concept_coverage\": concept_coverage,\n",
    "            \"retrieval_success\": retrieval_success,\n",
    "            \"length_score\": length_score,\n",
    "            \"no_answer_correct\": no_answer_correct,\n",
    "            \"composite_score\": composite_score,\n",
    "            \"avg_semantic_similarity\": avg_semantic_similarity\n",
    "        }\n",
    "        \n",
    "        improved_results.append(result_data)\n",
    "    \n",
    "    # Calcular estad√≠sticas mejoradas\n",
    "    avg_composite_improved = sum(r[\"composite_score\"] for r in improved_results) / len(improved_results)\n",
    "    avg_jaccard_improved = sum(r[\"jaccard_similarity\"] for r in improved_results) / len(improved_results)\n",
    "    retrieval_rate_improved = sum(r[\"retrieval_success\"] for r in improved_results) / len(improved_results)\n",
    "    avg_semantic_improved = sum(r[\"avg_semantic_similarity\"] for r in improved_results) / len(improved_results)\n",
    "    \n",
    "    print(f\"\\nüìä Resultados del Sistema Mejorado:\")\n",
    "    print(f\"Puntuaci√≥n compuesta promedio: {avg_composite_improved:.3f}\")\n",
    "    print(f\"Similitud Jaccard promedio: {avg_jaccard_improved:.3f}\")\n",
    "    print(f\"Tasa de recuperaci√≥n exitosa: {retrieval_rate_improved:.3f}\")\n",
    "    print(f\"Similitud sem√°ntica promedio: {avg_semantic_improved:.3f}\")\n",
    "    \n",
    "    # Comparar con resultados originales\n",
    "    if 'comprehensive_results' in globals():\n",
    "        original_avg = sum(r[\"composite_score\"] for r in comprehensive_results) / len(comprehensive_results)\n",
    "        original_jaccard = sum(r[\"jaccard_similarity\"] for r in comprehensive_results) / len(comprehensive_results)\n",
    "        original_retrieval = sum(r[\"retrieval_success\"] for r in comprehensive_results) / len(comprehensive_results)\n",
    "        \n",
    "        print(f\"\\nüìà Comparaci√≥n con Sistema Original:\")\n",
    "        print(f\"Mejora en puntuaci√≥n compuesta: {((avg_composite_improved - original_avg) / original_avg * 100):+.1f}%\")\n",
    "        print(f\"Mejora en similitud Jaccard: {((avg_jaccard_improved - original_jaccard) / original_jaccard * 100):+.1f}%\")\n",
    "        print(f\"Mejora en recuperaci√≥n: {((retrieval_rate_improved - original_retrieval) / original_retrieval * 100):+.1f}%\")\n",
    "    \n",
    "    return improved_results\n",
    "\n",
    "# Evaluar sistema mejorado\n",
    "improved_evaluation_results = evaluate_improved_system_complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A√±adir Ejemplos al Dataset\n",
    "\n",
    "Cada ejemplo consta de:\n",
    "- `inputs`: Un diccionario con las entradas de nuestro sistema (en este caso, la `query`).\n",
    "- `outputs`: Un diccionario con la salida de referencia que esperamos (la `answer` correcta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.create_example(\n",
    "    inputs={\"query\": \"¬øQu√© es la inteligencia artificial?\"},\n",
    "    outputs={\"answer\": \"La inteligencia artificial es una rama de la inform√°tica que busca crear m√°quinas capaces de realizar tareas que requieren inteligencia humana.\"},\n",
    "    dataset_id=dataset.id,\n",
    ")\n",
    "\n",
    "client.create_example(\n",
    "    inputs={\"query\": \"¬øPara qu√© sirve LangChain?\"},\n",
    "    outputs={\"answer\": \"LangChain es un framework que facilita el desarrollo de aplicaciones con modelos de lenguaje, proporcionando herramientas para cadenas y agentes.\"},\n",
    "    dataset_id=dataset.id,\n",
    ")\n",
    "\n",
    "client.create_example(\n",
    "    inputs={\"query\": \"Explica qu√© es RAG\"},\n",
    "    outputs={\"answer\": \"RAG (Retrieval-Augmented Generation) combina la b√∫squeda de informaci√≥n relevante con la generaci√≥n de texto para producir respuestas m√°s precisas.\"},\n",
    "    dataset_id=dataset.id,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ 3 ejemplos a√±adidos al dataset '{dataset_name}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. An√°lisis de Resultados\n",
    "\n",
    "Una vez completada la evaluaci√≥n, puedes navegar a la pesta√±a **Experiments** en tu proyecto de LangSmith.\n",
    "\n",
    "All√≠ encontrar√°s:\n",
    "1. Un resumen del experimento con las puntuaciones medias de cada m√©trica.\n",
    "2. Una tabla detallada con cada pregunta del dataset, la respuesta generada, la respuesta de referencia y las puntuaciones de la evaluaci√≥n.\n",
    "3. Para cada fila, puedes hacer clic para ver la traza completa y entender por qu√© el sistema respondi√≥ de esa manera (qu√© documentos recuper√≥, qu√© prompt se us√≥, etc.).\n",
    "\n",
    "Este an√°lisis te permite identificar puntos d√©biles. Por ejemplo:\n",
    "- **Puntuaciones bajas de `correctness`**: Puede que la recuperaci√≥n no est√© funcionando bien o que el prompt de generaci√≥n necesite ajustes.\n",
    "- **Documentos irrelevantes en el contexto**: Indica que el m√©todo de `retrieval` debe ser mejorado (por ejemplo, pasando de b√∫squeda por palabras clave a b√∫squeda sem√°ntica con embeddings)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusi√≥n\n",
    "\n",
    "La evaluaci√≥n es un pilar fundamental en el desarrollo de sistemas de IA robustos. LangSmith nos ofrece un conjunto de herramientas poderosas para automatizar este proceso en sistemas RAG, permiti√©ndonos pasar de un desarrollo basado en la intuici√≥n a uno guiado por datos y m√©tricas objetivas. Con este enfoque, podemos mejorar de forma iterativa la calidad y fiabilidad de nuestras aplicaciones."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
