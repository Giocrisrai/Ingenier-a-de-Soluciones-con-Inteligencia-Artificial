{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. RAG Completo con B√∫squeda Vectorial y LangChain\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "- Integrar el chunking, los embeddings y la recuperaci√≥n en un √∫nico pipeline de RAG.\n",
    "- Utilizar una base de datos vectorial (FAISS) para almacenar y buscar chunks de manera eficiente.\n",
    "- Construir una cadena `RetrievalQA` en LangChain para orquestar todo el proceso.\n",
    "- Realizar una consulta y obtener una respuesta generada por el LLM basada en el contexto recuperado.\n",
    "\n",
    "## El Pipeline de RAG Vectorial\n",
    "\n",
    "En los notebooks anteriores, preparamos los componentes: dividimos el texto y creamos embeddings. Ahora, vamos a unirlos en un sistema funcional. Este es el flujo de trabajo completo de un RAG basado en vectores:\n",
    "\n",
    "1.  **Indexaci√≥n (se hace una sola vez):**\n",
    "    - Se carga un documento.\n",
    "    - Se divide en chunks.\n",
    "    - Se generan embeddings para cada chunk.\n",
    "    - Los chunks y sus embeddings se almacenan en una **base de datos vectorial** (Vector Store).\n",
    "2.  **Recuperaci√≥n y Generaci√≥n (se hace para cada consulta):**\n",
    "    - El usuario hace una pregunta (consulta).\n",
    "    - Se genera un embedding para la consulta.\n",
    "    - Se usa el embedding de la consulta para buscar en la base de datos vectorial los chunks m√°s similares (b√∫squeda de similitud).\n",
    "    - Los chunks recuperados se pasan como contexto al LLM junto con la consulta original.\n",
    "    - El LLM genera una respuesta basada en el contexto proporcionado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install faiss\n",
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Modelos de embeddings y chat inicializados con LangChain.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# --- Configuraci√≥n del Cliente y Modelos de LangChain ---\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"GITHUB_TOKEN\")\n",
    "os.environ[\"OPENAI_API_BASE\"] = os.getenv(\"GITHUB_BASE_URL\", \"https://models.inference.ai.azure.com\")\n",
    "\n",
    "# Modelo de embeddings (compatible con la API de OpenAI)\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "# Modelo de lenguaje para la generaci√≥n de respuestas\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4.1\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "print(\"‚úì Modelos de embeddings y chat inicializados con LangChain.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carga y Fragmentaci√≥n del Documento\n",
    "\n",
    "Reutilizamos el mismo texto sobre la historia de la IA y lo dividimos en chunks, exactamente como en el notebook anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### üìú Documento dividido en 7 chunks."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "long_text = (\n",
    "    \"La historia de la inteligencia artificial (IA) es una narrativa fascinante de ambici√≥n, innovaci√≥n y perseverancia. \"\n",
    "    \"Sus ra√≠ces se remontan a la d√©cada de 1950, cuando pioneros como Alan Turing plantearon la pregunta de si las m√°quinas pod√≠an pensar. \"\n",
    "    \"El t√©rmino 'inteligencia artificial' fue acu√±ado por John McCarthy en 1956 en la famosa Conferencia de Dartmouth, considerada el nacimiento oficial de la IA como campo de estudio. \"\n",
    "    \"Los primeros a√±os estuvieron marcados por un gran optimismo, con la creaci√≥n de programas como el Logic Theorist y el General Problem Solver, que pod√≠an resolver problemas de l√≥gica y teoremas matem√°ticos. \"\n",
    "    \"Sin embargo, las limitaciones computacionales y la complejidad de los problemas del mundo real llevaron al primer 'invierno de la IA' en la d√©cada de 1970, un per√≠odo de reducci√≥n de fondos y escepticismo. \"\n",
    "    \"El resurgimiento lleg√≥ en la d√©cada de 1980 con el auge de los sistemas expertos, programas que encapsulaban el conocimiento de un experto humano en un dominio espec√≠fico, como el diagn√≥stico m√©dico (por ejemplo, MYCIN). \"\n",
    "    \"Estos sistemas demostraron el valor comercial de la IA, pero su fragilidad y el alto costo de mantenimiento condujeron a un segundo invierno a finales de los 80 y principios de los 90. \"\n",
    "    \"La revoluci√≥n moderna de la IA comenz√≥ a gestarse a finales de los 90 y principios de los 2000, impulsada por tres factores clave: la disponibilidad de grandes vol√∫menes de datos (Big Data), el desarrollo de hardware m√°s potente (especialmente las GPU) y los avances en algoritmos de aprendizaje autom√°tico, en particular las redes neuronales profundas (deep learning). \"\n",
    "    \"Hitos como la victoria de Deep Blue de IBM sobre el campe√≥n de ajedrez Garry Kasparov en 1997 y, m√°s tarde, el triunfo de AlphaGo de DeepMind en el juego de Go en 2016, demostraron el poder del aprendizaje por refuerzo y el deep learning. \"\n",
    "    \"Hoy, vivimos en la era de los modelos de lenguaje grande (LLM) como GPT y Claude, y los modelos de difusi√≥n para la generaci√≥n de im√°genes, que han llevado la IA a la corriente principal, transformando industrias y planteando nuevas preguntas sobre el futuro de la tecnolog√≠a y la humanidad.\"\n",
    ")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=350,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len\n",
    ")\n",
    "chunks = text_splitter.split_text(long_text)\n",
    "\n",
    "display(Markdown(f\"### üìú Documento dividido en {len(chunks)} chunks.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creaci√≥n de la Base de Datos Vectorial con FAISS\n",
    "\n",
    "Aqu√≠ es donde la magia ocurre. LangChain simplifica enormemente la creaci√≥n de la base de datos vectorial.\n",
    "\n",
    "Usamos `FAISS.from_texts()`, que realiza los siguientes pasos internamente:\n",
    "1.  Toma nuestra lista de `chunks`.\n",
    "2.  Utiliza el modelo de `embeddings` que le proporcionamos para convertir cada chunk en un vector.\n",
    "3.  Crea un √≠ndice FAISS en memoria con todos los vectores y sus correspondientes chunks de texto.\n",
    "\n",
    "**FAISS (Facebook AI Similarity Search)** es una librer√≠a altamente optimizada para la b√∫squeda de similitud en conjuntos masivos de vectores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Base de datos vectorial FAISS creada en memoria.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Crear la base de datos vectorial a partir de los chunks y el modelo de embeddings\n",
    "    vector_db = FAISS.from_texts(texts=chunks, embedding=embeddings)\n",
    "    print(\"‚úì Base de datos vectorial FAISS creada en memoria.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error al crear la base de datos vectorial: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Consulta y Recuperaci√≥n de Chunks Relevantes\n",
    "\n",
    "Antes de pasar al LLM, veamos qu√© recupera nuestro sistema. Un `retriever` es un componente de LangChain que, dada una consulta, devuelve los documentos m√°s relevantes desde la base de datos vectorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### üîç Chunks recuperados para la consulta: *'¬øQui√©n acu√±√≥ el t√©rmino 'inteligencia artificial'?'*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CHUNK RELEVANTE 1 ---\n",
      "La historia de la inteligencia artificial (IA) es una narrativa fascinante de ambici√≥n, innovaci√≥n y perseverancia. Sus ra√≠ces se remontan a la d√©cada de 1950, cuando pioneros como Alan Turing plantearon la pregunta de si las m√°quinas pod√≠an pensar. El t√©rmino 'inteligencia artificial' fue acu√±ado por John McCarthy en 1956 en la famosa Conferencia\n",
      "\n",
      "--- CHUNK RELEVANTE 2 ---\n",
      "John McCarthy en 1956 en la famosa Conferencia de Dartmouth, considerada el nacimiento oficial de la IA como campo de estudio. Los primeros a√±os estuvieron marcados por un gran optimismo, con la creaci√≥n de programas como el Logic Theorist y el General Problem Solver, que pod√≠an resolver problemas de l√≥gica y teoremas matem√°ticos. Sin embargo, las\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"¬øQui√©n acu√±√≥ el t√©rmino 'inteligencia artificial'?\"\n",
    "\n",
    "if 'vector_db' in locals():\n",
    "    # El retriever es la interfaz para buscar en la base de datos\n",
    "    retriever = vector_db.as_retriever(search_kwargs={\"k\": 2}) # k=2 para obtener los 2 chunks m√°s relevantes\n",
    "    \n",
    "    # Realizar la b√∫squeda\n",
    "    relevant_chunks = retriever.invoke(query)\n",
    "    \n",
    "    display(Markdown(f\"### üîç Chunks recuperados para la consulta: *'{query}'*\"))\n",
    "    for i, chunk in enumerate(relevant_chunks):\n",
    "        print(f\"--- CHUNK RELEVANTE {i+1} ---\")\n",
    "        print(chunk.page_content)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generaci√≥n de la Respuesta con `RetrievalQA`\n",
    "\n",
    "Ahora, unimos todo. La cadena `RetrievalQA` de LangChain est√° dise√±ada exactamente para este prop√≥sito. Le proporcionamos:\n",
    "\n",
    "-   `llm`: El modelo de lenguaje que generar√° la respuesta final.\n",
    "-   `retriever`: Nuestro recuperador de la base de datos FAISS.\n",
    "-   `chain_type=\"stuff\"`: Esta es la estrategia m√°s simple. Simplemente \"rellena\" (stuff) el prompt con todos los chunks recuperados.\n",
    "\n",
    "La cadena se encargar√° de todo el proceso: tomar la consulta, recuperar los chunks, construir el prompt y obtener la respuesta del LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### üí¨ Respuesta Generada por el LLM"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "El t√©rmino 'inteligencia artificial' fue acu√±ado por John McCarthy en 1956 durante la famosa Conferencia de Dartmouth."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if 'vector_db' in locals():\n",
    "    # Crear la cadena de RetrievalQA\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vector_db.as_retriever()\n",
    "    )\n",
    "    \n",
    "    # Ejecutar la cadena con nuestra consulta\n",
    "    response = qa_chain.invoke({\"query\": query})\n",
    "    \n",
    "    display(Markdown(f\"### üí¨ Respuesta Generada por el LLM\"))\n",
    "    display(Markdown(response['result']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusiones\n",
    "\n",
    "¬°Felicidades! Has construido un sistema de RAG completo y funcional. \n",
    "\n",
    "- **Automatizaci√≥n**: LangChain ha orquestado todos los pasos, desde la creaci√≥n de la base de datos vectorial hasta la generaci√≥n de la respuesta final, con muy poco c√≥digo.\n",
    "- **Precisi√≥n**: La respuesta del LLM se basa directamente en la informaci√≥n encontrada en el documento, lo que la hace precisa y fiable, evitando alucinaciones.\n",
    "- **Eficiencia**: FAISS permite que la b√∫squeda de similitud sea extremadamente r√°pida, incluso con millones de documentos.\n",
    "\n",
    "Este es el patr√≥n fundamental sobre el que se construyen la mayor√≠a de las aplicaciones de RAG modernas. A partir de aqu√≠, se pueden explorar t√©cnicas m√°s avanzadas como diferentes estrategias de chunking, re-ranking de resultados o cadenas m√°s complejas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### ‚ùì Consulta: *¬øQu√© caus√≥ el primer invierno de la IA?*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### üí¨ Respuesta: *El primer \"invierno de la IA\" fue causado principalmente por las limitaciones computacionales y la complejidad de los problemas del mundo real. Aunque los primeros programas de IA, como el Logic Theorist y el General Problem Solver, lograron resolver problemas de l√≥gica y teoremas matem√°ticos, pronto se hizo evidente que estos enfoques no pod√≠an escalar ni manejar situaciones m√°s complejas fuera de entornos controlados. Esto llev√≥ a una reducci√≥n de fondos y un aumento del escepticismo en la d√©cada de 1970, marcando as√≠ el primer invierno de la IA.*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Probemos con otra pregunta para ver la robustez del sistema\n",
    "if 'qa_chain' in locals():\n",
    "    query_invierno = \"¬øQu√© caus√≥ el primer invierno de la IA?\"\n",
    "    response_invierno = qa_chain.invoke({\"query\": query_invierno})\n",
    "    \n",
    "    display(Markdown(f\"### ‚ùì Consulta: *{query_invierno}*\"))\n",
    "    display(Markdown(f\"### üí¨ Respuesta: *{response_invierno['result']}*\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
