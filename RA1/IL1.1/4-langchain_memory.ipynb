{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bmsvkxtyalo",
   "metadata": {},
   "source": [
    "# 4. LangChain Memory - Gestión de Contexto Conversacional\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "- Comprender la importancia de la memoria en conversaciones con LLMs\n",
    "- Implementar diferentes tipos de memoria con LangChain\n",
    "- Gestionar el contexto de conversaciones largas\n",
    "- Optimizar el uso de tokens con estrategias de memoria\n",
    "\n",
    "## ¿Por qué es Importante la Memoria?\n",
    "\n",
    "Los LLMs son **stateless** por naturaleza: no recuerdan conversaciones anteriores. La memoria permite:\n",
    "- **Contexto conversacional**: Referirse a mensajes anteriores\n",
    "- **Personalización**: Recordar preferencias del usuario\n",
    "- **Continuidad**: Mantener hilos de conversación coherentes\n",
    "- **Experiencia natural**: Conversaciones que se sienten humanas\n",
    "\n",
    "## Tipos de Memoria en LangChain\n",
    "\n",
    "1. **ConversationBufferMemory**: Mantiene todo el historial\n",
    "2. **ConversationSummaryMemory**: Resume conversaciones largas\n",
    "3. **ConversationBufferWindowMemory**: Mantiene solo los N mensajes más recientes\n",
    "4. **ConversationSummaryBufferMemory**: Combina resumen + buffer reciente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ac0c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar bibliotecas necesarias para memoria\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import (\n",
    "    ConversationBufferMemory,\n",
    "    ConversationSummaryMemory,\n",
    "    ConversationBufferWindowMemory\n",
    ")\n",
    "from langchain_core.chat_history import BaseChatMessageHistory, InMemoryChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "import os\n",
    "\n",
    "print(\"✓ Bibliotecas de memoria importadas correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8f3673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración del modelo para memoria\n",
    "try:\n",
    "    llm = ChatOpenAI(\n",
    "        base_url=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "        api_key=os.getenv(\"GITHUB_TOKEN\"),\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    print(\"✓ Modelo configurado para experimentos de memoria\")\n",
    "    print(f\"Modelo: {llm.model_name}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error en configuración: {e}\")\n",
    "    print(\"Verifica las variables de entorno\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe7eab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt con historial + entrada del usuario\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un asistente útil.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Cadena = prompt + modelo\n",
    "chain = prompt | llm\n",
    "\n",
    "# Almacén de historiales\n",
    "store = {}\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# Envolver con memoria\n",
    "conversation = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc98272",
   "metadata": {},
   "source": [
    "## 1. ConversationBufferMemory - Memoria Completa\n",
    "\n",
    "Esta memoria mantiene **todo** el historial de la conversación. Es la más simple pero puede consumir muchos tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fb2837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo básico con RunnableWithMessageHistory\n",
    "\n",
    "# Prompt con historial + entrada\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un asistente útil.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Cadena = prompt + modelo\n",
    "chain = prompt | llm\n",
    "\n",
    "# Almacén de memorias por sesión\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str):\n",
    "    \"\"\"Devuelve (o crea) el historial completo para la sesión.\"\"\"\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# Envolver con RunnableWithMessageHistory\n",
    "conversation = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "def ejemplo_buffer_memory():\n",
    "    print(\"=== CONVERSATIONBUFFERMEMORY ===\")\n",
    "    print(\"Mantiene todo el historial de conversación\\n\")\n",
    "    \n",
    "    session_id = \"demo_session\"\n",
    "\n",
    "    try:\n",
    "        # Primera interacción\n",
    "        print(\"1. Primera pregunta:\")\n",
    "        response1 = conversation.invoke(\n",
    "            {\"input\": \"Mi nombre es Ana y soy programadora Python\"},\n",
    "            config={\"configurable\": {\"session_id\": session_id}}\n",
    "        )\n",
    "        print(f\"Respuesta: {response1.content}\\n\")\n",
    "\n",
    "        # Segunda interacción\n",
    "        print(\"2. Segunda pregunta:\")\n",
    "        response2 = conversation.invoke(\n",
    "            {\"input\": \"¿Cuál es mi nombre y profesión?\"},\n",
    "            config={\"configurable\": {\"session_id\": session_id}}\n",
    "        )\n",
    "        print(f\"Respuesta: {response2.content}\\n\")\n",
    "\n",
    "        # Tercera interacción\n",
    "        print(\"3. Tercera pregunta:\")\n",
    "        response3 = conversation.invoke(\n",
    "            {\"input\": \"¿Qué lenguaje de programación mencioné?\"},\n",
    "            config={\"configurable\": {\"session_id\": session_id}}\n",
    "        )\n",
    "        print(f\"Respuesta: {response3.content}\\n\")\n",
    "\n",
    "        # Mostrar historial\n",
    "        print(\"=== CONTENIDO DE LA MEMORIA ===\")\n",
    "        history = store[session_id].messages\n",
    "        for i, msg in enumerate(history, 1):\n",
    "            print(f\"{i}. {msg.type}: {msg.content}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Ejecutar\n",
    "ejemplo_buffer_memory()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1acba3",
   "metadata": {},
   "source": [
    "## 2. ConversationBufferWindowMemory - Ventana Deslizante\n",
    "\n",
    "Esta memoria mantiene solo los **N mensajes más recientes**, útil para controlar el uso de tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46248c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt con historial + entrada\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un asistente útil.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Cadena = prompt + modelo\n",
    "chain = prompt | llm\n",
    "\n",
    "# Almacén de memorias por sesión\n",
    "store = {}\n",
    "\n",
    "class WindowChatMessageHistory(BaseChatMessageHistory):\n",
    "    \"\"\"Historial de chat que mantiene solo los últimos k intercambios.\"\"\"\n",
    "    \n",
    "    def __init__(self, k: int = 2):\n",
    "        self.k = k\n",
    "        self._messages = []\n",
    "    \n",
    "    @property\n",
    "    def messages(self):\n",
    "        # Mantener solo los últimos k intercambios (k*2 mensajes: user + assistant)\n",
    "        return self._messages[-(self.k * 2):]\n",
    "    \n",
    "    def add_message(self, message):\n",
    "        self._messages.append(message)\n",
    "    \n",
    "    def clear(self):\n",
    "        self._messages.clear()\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    \"\"\"Devuelve el historial de ventana para la sesión.\"\"\"\n",
    "    if session_id not in store:\n",
    "        store[session_id] = WindowChatMessageHistory(k=2)\n",
    "    return store[session_id]\n",
    "\n",
    "# Envolver con RunnableWithMessageHistory\n",
    "conversation = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "# Ejemplo\n",
    "def ejemplo_window_memory():\n",
    "    print(\"=== CONVERSATION BUFFER WINDOW MEMORY (k=2) ===\")\n",
    "    print(\"Mantiene solo los 2 intercambios más recientes\\n\")\n",
    "    \n",
    "    session_id = \"demo_window\"\n",
    "    inputs = [\n",
    "        \"Mi nombre es Carlos y tengo 30 años\",\n",
    "        \"Trabajo como diseñador gráfico\", \n",
    "        \"Me gusta el café y la música jazz\",\n",
    "        \"¿Puedes recordar mi edad?\",\n",
    "        \"¿Cuál es mi profesión?\"\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        for i, user_input in enumerate(inputs, 1):\n",
    "            print(f\"{'='*20} INTERACCIÓN {i} {'='*20}\")\n",
    "            print(f\"👤 Usuario: {user_input}\")\n",
    "            \n",
    "            response = conversation.invoke(\n",
    "                {\"input\": user_input},\n",
    "                config={\"configurable\": {\"session_id\": session_id}}\n",
    "            )\n",
    "            print(f\"🤖 Asistente: {response.content}\\n\")\n",
    "            \n",
    "            # Obtener el historial\n",
    "            history = get_session_history(session_id)\n",
    "            \n",
    "            # Mostrar comparación clara\n",
    "            total_messages = len(history._messages)\n",
    "            visible_messages = len(history.messages)\n",
    "            \n",
    "            print(f\"📊 ESTADO DE LA MEMORIA:\")\n",
    "            print(f\"   💾 Total almacenado: {total_messages} mensajes\")\n",
    "            print(f\"   👁️  Visible al modelo: {visible_messages} mensajes\")\n",
    "            print(f\"   🗑️  Mensajes descartados: {total_messages - visible_messages}\")\n",
    "            \n",
    "            # Mensajes almacenados totalmente\n",
    "            print(f\"\\n📚 HISTORIAL COMPLETO ALMACENADO ({total_messages} mensajes):\")\n",
    "            if total_messages == 0:\n",
    "                print(\"     (Ningún mensaje aún)\")\n",
    "            else:\n",
    "                for j, msg in enumerate(history._messages, 1):\n",
    "                    role = \"👤 Usuario\" if msg.type == \"human\" else \"🤖 Asistente\"\n",
    "                    content = msg.content[:60] + \"...\" if len(msg.content) > 60 else msg.content\n",
    "                    # Marcar si está en la ventana visible\n",
    "                    is_visible = j > total_messages - visible_messages\n",
    "                    marker = \"✅\" if is_visible else \"❌\"\n",
    "                    print(f\"     {j}. {marker} {role}: {content}\")\n",
    "            \n",
    "            # Lo que ve el modelo\n",
    "            print(f\"\\n🔍 VENTANA VISIBLE AL MODELO ({visible_messages} mensajes):\")\n",
    "            if visible_messages == 0:\n",
    "                print(\"     (Ningún mensaje visible)\")\n",
    "            else:\n",
    "                for j, msg in enumerate(history.messages, 1):\n",
    "                    role = \"👤 Usuario\" if msg.type == \"human\" else \"🤖 Asistente\"\n",
    "                    content = msg.content[:60] + \"...\" if len(msg.content) > 60 else msg.content\n",
    "                    print(f\"     {j}. ✅ {role}: {content}\")\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Ejecutar\n",
    "ejemplo_window_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2195b2",
   "metadata": {},
   "source": [
    "## 3. ConversationSummaryMemory - Resumen Inteligente\n",
    "\n",
    "Esta memoria **resume** conversaciones largas en lugar de mantener todo el texto completo, ahorrando tokens significativamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a929543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_session_history(session_id: str):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "# Función para resumir automáticamente cuando hay muchos mensajes\n",
    "def auto_summarize(session_id: str, max_messages=6):\n",
    "    history = get_session_history(session_id)\n",
    "    \n",
    "    if len(history.messages) > max_messages:\n",
    "        # Mensajes a resumir (todos excepto los últimos 2)\n",
    "        messages_to_summarize = history.messages[:-2]\n",
    "        \n",
    "        # Crear texto para resumir\n",
    "        conversation_text = \"\"\n",
    "        for msg in messages_to_summarize:\n",
    "            role = \"Usuario\" if msg.type == \"human\" else \"Asistente\"\n",
    "            conversation_text += f\"{role}: {msg.content}\\n\"\n",
    "        \n",
    "        # Generar resumen\n",
    "        summary_response = llm.invoke(f\"Resume esta conversación en 2-3 líneas:\\n{conversation_text}\")\n",
    "        summary = summary_response.content\n",
    "        \n",
    "        # Reemplazar mensajes antiguos con el resumen\n",
    "        recent_messages = history.messages[-2:]\n",
    "        history.clear()\n",
    "        history.add_ai_message(f\"[RESUMEN]: {summary}\")\n",
    "        history.messages.extend(recent_messages)\n",
    "\n",
    "# Crear conversación\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Eres un asistente útil.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "conversation = RunnableWithMessageHistory(\n",
    "    prompt | llm,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "def ejemplo_summary_memory():\n",
    "    print(\"=== CONVERSATION SUMMARY MEMORY ===\")\n",
    "    print(\"Resume conversaciones largas para ahorrar tokens\\n\")\n",
    "    \n",
    "    session_id = \"summary_session\"\n",
    "    \n",
    "    # Conversación de ejemplo\n",
    "    inputs = [\n",
    "        \"Hola, soy María González, ingeniera de software de 35 años\",\n",
    "        \"Trabajo en una startup de fintech en Madrid desarrollando pagos digitales\",\n",
    "        \"Usamos React, Node.js, Docker y Kubernetes en nuestros proyectos\",\n",
    "        \"Mi mayor desafío es la latencia en transacciones internacionales\",\n",
    "        \"También trabajo en mejorar la UX de nuestra app móvil\",\n",
    "        \"¿Puedes resumir quién soy y cuáles son mis principales desafíos?\"\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        for i, user_input in enumerate(inputs, 1):\n",
    "            print(f\"{'='*15} INTERACCIÓN {i} {'='*15}\")\n",
    "            print(f\"👤 Usuario: {user_input}\")\n",
    "            \n",
    "            # Resumir automáticamente si es necesario\n",
    "            auto_summarize(session_id)\n",
    "            \n",
    "            response = conversation.invoke(\n",
    "                {\"input\": user_input},\n",
    "                config={\"configurable\": {\"session_id\": session_id}}\n",
    "            )\n",
    "            print(f\"🤖 Asistente: {response.content}\\n\")\n",
    "            \n",
    "            # Mostrar estado de la memoria\n",
    "            history = get_session_history(session_id)\n",
    "            total_messages = len(history.messages)\n",
    "            \n",
    "            print(f\"📊 ESTADO DE LA MEMORIA:\")\n",
    "            print(f\"   💾 Total mensajes: {total_messages}\")\n",
    "            \n",
    "            # Verificar si hay resumen\n",
    "            has_summary = any(\"[RESUMEN]\" in msg.content for msg in history.messages if hasattr(msg, 'content'))\n",
    "            print(f\"   📝 Tiene resumen: {'✅ Sí' if has_summary else '❌ No'}\")\n",
    "            \n",
    "            print(f\"\\n💬 CONTENIDO ACTUAL DE LA MEMORIA:\")\n",
    "            for j, msg in enumerate(history.messages, 1):\n",
    "                role = \"👤 Usuario\" if msg.type == \"human\" else \"🤖 Asistente\"\n",
    "                content = msg.content\n",
    "                \n",
    "                # Destacar si es un resumen\n",
    "                if \"[RESUMEN]\" in content:\n",
    "                    role = \"📝 Resumen\"\n",
    "                    content = content.replace(\"[RESUMEN]: \", \"\")\n",
    "                \n",
    "                # Truncar si es muy largo\n",
    "                if len(content) > 80:\n",
    "                    content = content[:80] + \"...\"\n",
    "                \n",
    "                print(f\"   {j}. {role}: {content}\")\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Ejecutar\n",
    "ejemplo_summary_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c85cfea",
   "metadata": {},
   "source": [
    "## Consideraciones Técnicas y Mejores Prácticas\n",
    "\n",
    "### Selección del Tipo de Memoria\n",
    "\n",
    "| Tipo | Cuándo Usarlo | Ventajas | Desventajas |\n",
    "|------|---------------|----------|-------------|\n",
    "| **Buffer** | Conversaciones cortas | Contexto completo | Alto consumo de tokens |\n",
    "| **Window** | Contexto reciente importante | Eficiente en tokens | Puede perder información clave |\n",
    "| **Summary** | Conversaciones muy largas | Balance eficiencia/contexto | Pérdida de detalles específicos |\n",
    "\n",
    "### Mejores Prácticas:\n",
    "\n",
    "1. **Gestión de Tokens**:\n",
    "   - Monitorea el uso de tokens regularmente\n",
    "   - Establece límites máximos para evitar costos excesivos\n",
    "   - Considera el costo vs. calidad del contexto\n",
    "\n",
    "2. **Selección Estratégica**:\n",
    "   - Usa Buffer para sesiones cortas e importantes\n",
    "   - Usa Window para conversaciones con contexto limitado\n",
    "   - Usa Summary para sesiones largas de asistencia\n",
    "\n",
    "3. **Optimización**:\n",
    "   - Limpia memoria periódicamente si es necesario\n",
    "   - Implementa estrategias híbridas según el caso de uso\n",
    "   - Considera almacenamiento persistente para memoria a largo plazo\n",
    "\n",
    "## Ejercicios Prácticos\n",
    "\n",
    "### Ejercicio 1: Análisis de Consumo\n",
    "Implementa un sistema que monitoree y reporte el uso de tokens con diferentes tipos de memoria.\n",
    "\n",
    "### Ejercicio 2: Memoria Híbrida\n",
    "Diseña una estrategia que combine multiple tipos de memoria según el contexto.\n",
    "\n",
    "### Ejercicio 3: Persistencia\n",
    "Extiende el chatbot para guardar y cargar memoria entre sesiones.\n",
    "\n",
    "## Conceptos Clave Aprendidos\n",
    "\n",
    "1. **Importancia de la memoria** en conversaciones naturales\n",
    "2. **Tipos de memoria** y sus casos de uso específicos\n",
    "3. **Balance** entre contexto y eficiencia de tokens\n",
    "4. **Implementación práctica** con LangChain\n",
    "5. **Estrategias de optimización** para diferentes escenarios\n",
    "\n",
    "## Conclusión del Módulo IL1.1\n",
    "\n",
    "Has completado la introducción a LLMs y conexiones API. Los conceptos aprendidos:\n",
    "\n",
    "1. **APIs directas** vs **frameworks** como LangChain\n",
    "2. **Streaming** para mejor experiencia de usuario\n",
    "3. **Memoria** para conversaciones contextuales\n",
    "4. **Mejores prácticas** de seguridad y optimización\n",
    "\n",
    "### Próximos Pasos\n",
    "En **IL1.2** exploraremos técnicas avanzadas de **prompt engineering** incluyendo zero-shot, few-shot, y chain-of-thought prompting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
